# -*- coding: utf-8 -*-
"""Full code course work.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17GclCz2M-3Uq8KOo38e4FT_EHfKmzvvS

# Аффективная поляризация
"""

!pip install -q ruptures bertopic sentence-transformers

import pandas as pd
import numpy as np
import re
from datetime import datetime, timedelta
from dateutil import parser
from collections import Counter
from tqdm import tqdm
tqdm.pandas()

import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import seaborn as sns
from wordcloud import WordCloud
from pandas.plotting import autocorrelation_plot

import statsmodels.api as sm
from statsmodels.tsa.stattools import adfuller, grangercausalitytests, arma_order_select_ic
from statsmodels.stats.stattools import durbin_watson
from statsmodels.stats.diagnostic import het_breuschpagan
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.ar_model import AutoReg
from statsmodels.tsa.api import VAR

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.metrics import silhouette_score

from sentence_transformers import SentenceTransformer
from bertopic import BERTopic
from cuml.manifold import UMAP as cuUMAP
from hdbscan import HDBSCAN

import warnings
warnings.filterwarnings('ignore')

# Тема оформления
sns.set_theme(style="whitegrid")

from google.colab import drive; drive.mount('/content/drive')

folder = '/content/drive/MyDrive/polarization_data/'

df_tweets = pd.read_csv(folder + 'dataset.csv')
df_tweets.head()

"""Достаем дату из твита, исходя из snowflake подхода"""

TWITTER_EPOCH = 1288834974657

def extract_tweet_id(url: str) -> int:
    match = re.search(r'/status/(\d+)', url)
    if not match:
        raise ValueError(f"Не удалось извлечь ID из {url}")
    return int(match.group(1))

def snowflake_to_datetime(tweet_id: int, tz_offset_hours: int = 3) -> datetime:
    timestamp_ms = (tweet_id >> 22) + TWITTER_EPOCH
    dt_utc = datetime.utcfromtimestamp(timestamp_ms / 1000.0)
    return dt_utc + timedelta(hours=tz_offset_hours)

df_tweets['id'] = df_tweets['link'].apply(extract_tweet_id)
df_tweets['datetime'] = df_tweets['id'].apply(lambda tid: snowflake_to_datetime(tid, tz_offset_hours=3))

"""## EDA"""

df_tweets.isna().sum()

df_tweets.dropna(inplace=True)

df_tweets.describe()

df_tweets["hour"] = df_tweets["datetime"].dt.hour
sns.countplot(x="hour", data=df_tweets)
plt.title("Твиты по часам суток")
plt.show()

hourly_counts = df_tweets["hour"].value_counts().sort_index()

plt.figure(figsize=(10, 5), dpi=120)


ax = sns.barplot(
    x=hourly_counts.index,
    y=hourly_counts.values,
    edgecolor='black'
)

for p in ax.patches:
    height = p.get_height()
    ax.annotate(
        f"{int(height)}",
        (p.get_x() + p.get_width() / 2, height),
        ha='center',
        va='bottom',
        fontsize=9,
        color='black'
    )

ax.set_title("Активность в Твиттере по часам суток\n", fontsize=16)
ax.set_xlabel("Час суток", fontsize=12)
ax.set_ylabel("Число твитов", fontsize=12)
ax.set_xticklabels([str(h) for h in hourly_counts.index], rotation=0)

plt.tight_layout()
plt.show()

weekly_counts = (
    df_tweets
    .set_index("datetime")["id"]
    .resample("W")
    .count()
)

max_date = weekly_counts.idxmax()
max_value = weekly_counts.max()

plt.figure(figsize=(12, 6))
plt.plot(
    weekly_counts.index,
    weekly_counts.values,
    marker='o',
    linestyle='-',
    linewidth=1,
    markersize=4
)

plt.annotate(
    f'{max_value}',
    xy=(max_date, max_value),
    xytext=(0, 10),
    textcoords='offset points',
    ha='center'
)

plt.title("Число твитов по неделям\n")
plt.ylabel("Количество твитов")
plt.xlabel("Дата")
plt.xticks(rotation=30)

plt.tight_layout()
plt.show()

all_text = " ".join(df_tweets["cleaned_text"].dropna())
wc = WordCloud(width=800,
               height=400,
               # random_state=42,
               background_color="white").generate(all_text)
plt.figure(figsize=(10,5))
plt.imshow(wc, interpolation="bilinear")
plt.axis("off")
plt.title("Самые популярные слова твитов")
plt.show()

plt.figure(figsize=(12, 8))

points = sns.scatterplot(
    x="likes",
    y="comments",
    data=df_tweets,
    alpha=0.4,
    edgecolor=None,
    hue="likes",
    palette="viridis",
    legend=False,
    s=50
)

points.set(xscale="log", yscale="log")
points.set_xlabel("Лайки (лог)", fontsize=14)
points.set_ylabel("Комментарии (лог)", fontsize=14)
points.set_title("Зависимость количества комментариев от лайков", fontsize=18)
points.set_ylim(bottom=-10)

x = np.log1p(df_tweets["likes"])
y = np.log1p(df_tweets["comments"])
coef = np.polyfit(x, y, 1)
poly1d_fn = np.poly1d(coef)
xs = np.linspace(x.min(), x.max(), 100)
plt.plot(np.expm1(xs), np.expm1(poly1d_fn(xs)),
         color="steelblue", linestyle="--", linewidth=2, label="Линия тренда")

corr = df_tweets[["likes","comments"]].corr().iloc[0,1]
plt.text(
    0.05, 0.95,
    f"Pearson r = {corr:.2f}",
    transform=plt.gca().transAxes,
    fontsize=12,
    verticalalignment='top',
    bbox=dict(boxstyle="round,pad=0.3", facecolor="white", edgecolor="gray", alpha=0.8)
)

sns.despine(trim=True)

plt.legend()
plt.tight_layout()
plt.show()

plt.figure(figsize=(6,4))
sns.histplot(df_tweets[df_tweets["likes"] < 20]["likes"], bins=50)
plt.title("Распределение лайков (лог-масштаб)")
plt.show()

df_tweets['label'].value_counts()

def remap_label(x):
    # Нейтральные
    if x in (0, 2, 4):
        # Нейтральные
        return 1
    elif x == 1:
        # Палестина
        return 0
    elif x in (3, 5):
        # Израиль
        return 2

df_tweets['label_r'] = df_tweets['label'].apply(remap_label)

label_counts = df_tweets['label_r'].value_counts().sort_index()
labels_map = {
    0: 'Палестина',
    1: 'Нейтральные',
    2: 'Израиль',
}
order = list(labels_map.keys())
labels = [labels_map[i] for i in order]

plt.figure(figsize=(8, 5), dpi=120)
bars = sns.barplot(
    x=order,
    y=label_counts.values,
    palette='viridis',
    edgecolor='black'
)

bars.set_xticklabels(labels)

for p in bars.patches:
    height = p.get_height()
    bars.annotate(
        f'{int(height)}',
        (p.get_x() + p.get_width() / 2, height),
        ha='center',
        va='bottom',
        fontsize=10
    )

plt.title('Распределение меток твитов\n', fontsize=14)
plt.xlabel('Класс', fontsize=12)
plt.ylabel('Количество', fontsize=12)
plt.ylim(top=4300)
plt.tight_layout()
plt.show()

"""## Выделение мнений"""

umap_model = cuUMAP(
    n_neighbors=15,
    n_components=5,
    random_state=42
)

texts = df_tweets['cleaned_text'].tolist()

embedding_model = SentenceTransformer(
    "all-MiniLM-L6-v2",
    device="cuda"
)
topic_model = BERTopic(
    embedding_model=embedding_model,
    umap_model=umap_model,
    hdbscan_model=HDBSCAN(min_cluster_size=10),
    nr_topics="auto",
    low_memory=False,
    verbose=True
)


topics, probs = topic_model.fit_transform(texts)

df_tweets['topic'] = topics

topic_info = topic_model.get_topic_info()

topic_info = topic_info.sort_values('Count', ascending=False)

fig1 = topic_model.visualize_topics()
fig1.show()

fig2 = topic_model.visualize_barchart(top_n_topics=10, n_words=8)
fig2.show()

fig3 = topic_model.visualize_heatmap()
fig3.show()

fig4 = topic_model.visualize_hierarchy()
fig4.show()

docs = df_tweets['cleaned_text'].tolist()
timestamps = df_tweets['datetime'].dt.to_pydatetime().tolist()
topics = df_tweets['topic'].tolist()

topics_over_time = topic_model.topics_over_time(
    docs,
    timestamps,
    topics,
    nr_bins=30
)

fig5 = topic_model.visualize_topics_over_time(
    topics_over_time,
    top_n_topics=10,
)
fig5.show()

"""## Анализ аффективной поляризации

### Поляризация корпуса
"""

texts_pos = df_tweets.loc[df_tweets['label_r']==0, 'cleaned_text']
texts_neg = df_tweets.loc[df_tweets['label_r']==2,'cleaned_text']

cnt_pos = Counter(" ".join(texts_pos).split())
cnt_neg = Counter(" ".join(texts_neg).split())
V = len(set(cnt_pos) | set(cnt_neg))
alpha = 1.0

def word_score(w):
    return np.log((cnt_pos[w]+alpha)/(sum(cnt_pos.values())+alpha*V)) \
         - np.log((cnt_neg[w]+alpha)/(sum(cnt_neg.values())+alpha*V))

def tweet_polarity(text):
    return sum(word_score(w) for w in text.split())

df_tweets['lex_polarity'] = df_tweets['cleaned_text'].progress_apply(tweet_polarity)

df_tweets['lex_polarity'].describe()

"""### Распределение категории в течении времени"""

df = df_tweets.set_index('datetime').sort_index()

window = '7D'
roll = df.rolling(window=window, min_periods=1)

df_roll = pd.DataFrame({
    'mean_label':      roll['label'].mean(),
    'std_label':       roll['label'].std(),
    'extreme_ratio':   roll['label'].apply(lambda x: (x.abs()==1).mean()),
    'mean_lex_polarity': roll['lex_polarity'].mean(),
    'sum_likes':       roll['likes'].sum(),
    'sum_retweets':    roll['retweets'].sum(),
    'sum_quotes':      roll['quotes'].sum(),
    'sum_comments':    roll['comments'].sum(),
}).dropna()

X = df_roll.values
dates = df_roll.index

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

k = 3
km = KMeans(n_clusters=k, random_state=42).fit(X_scaled)
labels_km = km.labels_


sil_score = silhouette_score(X_scaled, labels_km)
print(f"Silhouette score: {sil_score:.3f}")

df_roll['cluster'] = labels_km
df_roll['pc1'], df_roll['pc2'] = X_pca.T

fig, axes = plt.subplots(2, 1, figsize=(10, 8), sharex=False)
fig.tight_layout(pad=4)

ax = axes[0]
ax.step(dates, labels_km, where='mid', linewidth=2)
ax.set_ylabel('Кластер', fontsize=12)
ax.set_title('Динамика кластеров (rolling 7D)', fontsize=14)
ax.set_yticks(range(k))
ax.grid(True, linestyle='--', alpha=0.5)

ax.xaxis.set_major_locator(mdates.AutoDateLocator())
ax.xaxis.set_major_formatter(mdates.ConciseDateFormatter(ax.xaxis.get_major_locator()))

ax = axes[1]
for ci in range(k):
    mask = labels_km == ci
    ax.scatter(
        X_pca[mask, 0], X_pca[mask, 1],
        label=f'Кластер {ci}',
        s=50, alpha=0.7, edgecolor='w'
    )
ax.set_xlabel('PC1', fontsize=12)
ax.set_ylabel('PC2', fontsize=12)
ax.set_title('Кластеры в пространстве главных компонент', fontsize=14)
ax.legend(title='Кластеры')
ax.grid(True, linestyle='--', alpha=0.5)

plt.show()

"""### Авторегрессионная модель"""

ts_daily = df_roll['mean_label'].resample('D').mean()
ts_daily = ts_daily.fillna(method='ffill')

adf_stat, adf_p, *_ = adfuller(ts_daily)
if adf_p > 0.05:
    ts_for_ar = ts_daily.diff().dropna()
else:
    ts_for_ar = ts_daily.copy()

res = arma_order_select_ic(ts_for_ar, max_ar=10, ic=['aic','bic'], trend='n')
p = res.aic_min_order[0]

model = AutoReg(ts_daily, lags=p, old_names=False).fit()
print(model.summary())

pred = model.predict(start=ts_daily.index[p], end=ts_daily.index[-1])
mse = ((ts_daily[p:] - pred[p:])**2).mean()
print(f"MSE = {mse:.5f}")

"""### Тест Грейнджера"""

df_daily = (
    df
    .resample('D')
    .agg({
        'id':    'count',
        'label': 'mean'
    })
    .rename(columns={'id':'tweet_count','label':'pol_mean'})
)


onehot = pd.get_dummies(
    df['label'],
    prefix='roll_topic'
)
df_topic_daily = onehot.resample('D').mean()

df_daily = df_daily.join(df_topic_daily).fillna(0)

cols = ['pol_mean', 'tweet_count'] + [c for c in df_daily.columns if c.startswith('roll_topic_')]
df_gc = df_daily[cols].dropna()

print("=== tweet_count -> pol_mean ===")
grangercausalitytests(df_gc[['pol_mean','tweet_count']], maxlag=7, verbose=True)

for topic_col in [c for c in df_gc.columns if c.startswith('roll_topic_')]:
    print(f"\n=== {topic_col} -> pol_mean ===")
    result = grangercausalitytests(df_gc[['pol_mean', topic_col]], maxlag=7, verbose=False)
    for lag, res in result.items():
        pval = res[0]['ssr_ftest'][1]
        print(f"lag={lag:>2}, p-value={pval:.3f}")

"""### VAR"""

topic_cols = [c for c in df_daily.columns if c.startswith('roll_topic_')]
top3 = (df_daily[topic_cols].mean().sort_values(ascending=False)
        .head(3).index.tolist())

model_df = df_daily[['pol_mean', 'tweet_count'] + top3].copy()

model_df_diff = model_df.diff().dropna()

maxlags = 10
sel = VAR(model_df_diff).select_order(maxlags)
p = sel.aic
print(sel.summary())

var = VAR(model_df_diff)
res = var.fit(p)
res.summary()

irf = res.irf(10)

fig = irf.plot(impulse=top3[0], response='pol_mean')
plt.suptitle(f"Импульс топика {top3[0]}")
plt.tight_layout()
plt.show()

irf.plot(impulse='roll_topic_0', response='pol_mean')
plt.show()

irf.plot(impulse='tweet_count', response='pol_mean')
plt.show()